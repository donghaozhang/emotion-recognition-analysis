# ğŸ­ Emotion Recognition Analysis Framework

A comprehensive multi-model emotion recognition evaluation system using state-of-the-art vision-language models on the EXPW (Expression in the Wild) dataset.

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Models](https://img.shields.io/badge/Models-2-orange.svg)

## ğŸ”— Live Demo

**[ğŸ“Š View Interactive Report](https://donghaozhang.github.io/emotion-recognition-analysis/)** - Multi-model comparison dashboard

## ğŸ† Performance Results

| Model | Accuracy | Macro F1 | Speed | Cost Efficiency |
|-------|----------|----------|-------|-----------------|
| **ğŸ¥‡ Gemini 2.5 Flash** | **67.2%** | **0.569** | **2.9s/img** | High tokens (1,392/img) |
| **ğŸ¥ˆ GLM-4.5V** | 30.5% | 0.303 | 4.5s/img | **Low tokens (379/img)** |

## ğŸ“ Project Structure

```
emotion-recognition-analysis/
â”œâ”€â”€ ğŸ“„ README.md                   # This file
â”œâ”€â”€ âš™ï¸ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ”§ .env                       # API configuration (not committed)
â”œâ”€â”€ ğŸ“‹ CLAUDE.md                  # Development guidelines
â”‚
â”œâ”€â”€ ğŸ“‚ src/                       # Source code
â”‚   â”œâ”€â”€ ğŸ“‚ analyzers/            # Model implementations
â”‚   â”‚   â”œâ”€â”€ ğŸ¤– glm_emotion_analyzer.py      # GLM-4.5V analyzer
â”‚   â”‚   â””â”€â”€ ğŸ¤– gemini_emotion_analyzer.py   # Gemini 2.5 Flash analyzer
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸš€ run_glm_analysis.py    # GLM analysis runner
â”‚   â”œâ”€â”€ ğŸš€ run_gemini_analysis.py # Gemini analysis runner
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ evaluation/           # Evaluation scripts
â”‚       â”œâ”€â”€ ğŸ“Š evaluate_glm_performance.py        # Single model evaluation
â”‚       â””â”€â”€ ğŸ“Š evaluate_multi_model_performance.py # Multi-model comparison
â”‚
â”œâ”€â”€ ğŸ“‚ data/                      # Dataset files
â”‚   â”œâ”€â”€ ğŸ“Š valid_set_1_test_for_inference_add_width_height.xlsx
â”‚   â””â”€â”€ ğŸ“Š expw_results_all_methods.csv
â”‚
â”œâ”€â”€ ğŸ“‚ results/                   # Analysis results
â”‚   â”œâ”€â”€ ğŸ“ˆ glm_emotion_results.json      # GLM predictions
â”‚   â”œâ”€â”€ ğŸ“ˆ glm_emotion_results.csv       # GLM summary
â”‚   â”œâ”€â”€ ğŸ“ˆ gemini_emotion_results.json   # Gemini predictions  
â”‚   â””â”€â”€ ğŸ“ˆ gemini_emotion_results.csv    # Gemini summary
â”‚
â”œâ”€â”€ ğŸ“‚ reports/                   # HTML reports & dashboards
â”‚   â”œâ”€â”€ ğŸ¨ multi_model_comparison_report.html    # Performance comparison
â”‚   â”œâ”€â”€ ğŸ¨ multi_model_emotion_report.html       # Interactive dashboard
â”‚   â”œâ”€â”€ ğŸ¨ glm_performance_report.html           # GLM detailed report
â”‚   â””â”€â”€ ğŸ¨ emotion_analysis_dashboard.html       # Dataset overview
â”‚
â”œâ”€â”€ ğŸ“‚ dfew_128/                  # Test images (128 EXPW samples)
â””â”€â”€ ğŸ“‚ archive/                   # Historical data & backups
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/donghaozhang/emotion-recognition-analysis.git
cd emotion-recognition-analysis

# Create conda environment
conda create -n emotion_analysis python=3.9 -y
conda activate emotion_analysis

# Install dependencies
pip install -r requirements.txt
```

### 2. API Configuration

Get your OpenRouter API key from [openrouter.ai/keys](https://openrouter.ai/keys) and create `.env` file:

```bash
# .env file
OPENROUTER_API_KEY=your_api_key_here
```

### 3. Run Analysis

```bash
# Run GLM-4.5V analysis
cd src
python run_glm_analysis.py

# Run Gemini 2.5 Flash analysis  
python run_gemini_analysis.py

# Compare both models
cd evaluation
python evaluate_multi_model_performance.py
```

## ğŸ“Š Models Comparison

### GLM-4.5V
- **Accuracy**: 30.5%
- **Speed**: 4.5s per image
- **Cost**: 379 tokens per image (73% cheaper)
- **Best for**: Cost-sensitive applications

### Gemini 2.5 Flash Image Preview  
- **Accuracy**: 67.2% (2.2x better)
- **Speed**: 2.9s per image (55% faster)
- **Cost**: 1,392 tokens per image
- **Best for**: High-accuracy requirements

## ğŸ¯ Emotion Categories

The system recognizes 7 emotion categories:
- **Angry** ğŸ˜ 
- **Disgust** ğŸ¤¢  
- **Fear** ğŸ˜¨
- **Happy** ğŸ˜Š
- **Neutral** ğŸ˜
- **Sad** ğŸ˜¢
- **Surprise** ğŸ˜²

## ğŸ“ˆ Dataset Information

- **Source**: EXPW (Expression in the Wild) dataset
- **Test Set**: 128 carefully selected facial expression images
- **Format**: JPG images, ~640x605 resolution
- **Ground Truth**: Human-annotated emotion labels
- **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score, Confusion Matrix

## ğŸ”§ Development

### Adding New Models

1. Create analyzer in `src/analyzers/new_model_analyzer.py`
2. Implement required interface:
   ```python
   def analyze_emotion(self, image_path: Path) -> Dict
   def analyze_batch(self, image_folder: Path, num_images: int) -> List[Dict]
   def save_results(self, results: List[Dict], output_file: str)
   ```
3. Add to evaluation framework in `evaluate_multi_model_performance.py`
4. Create runner script in `src/run_new_model_analysis.py`

### Key Features

- **ğŸ”„ Real-time Progress Monitoring**: Live updates during batch processing
- **ğŸ“Š Comprehensive Metrics**: Accuracy, F1-scores, confusion matrices
- **ğŸ¨ Interactive Reports**: HTML dashboards with charts and visualizations  
- **âš¡ Parallel Processing**: Efficient batch analysis with rate limiting
- **ğŸ”’ Error Handling**: Robust error recovery and logging
- **ğŸ“ Organized Structure**: Clean, maintainable codebase

## ğŸ“‹ Requirements

```txt
openai>=1.3.0        # OpenRouter API access
pillow>=9.0.0        # Image processing
python-dotenv>=0.19.0 # Environment variables
pandas>=1.5.0        # Data manipulation
numpy>=1.21.0        # Numerical operations
matplotlib>=3.5.0    # Plotting
seaborn>=0.11.0      # Statistical visualization
scikit-learn>=1.1.0  # Machine learning metrics
openpyxl>=3.0.0      # Excel file handling
```

## ğŸ¨ Reports & Visualizations

The framework generates several types of reports:

1. **ğŸ“Š Multi-Model Comparison**: Side-by-side performance analysis
2. **ğŸ¯ Confusion Matrices**: Per-model accuracy breakdown by emotion
3. **ğŸ“ˆ Performance Metrics**: Precision, recall, F1-scores
4. **â±ï¸ Speed & Cost Analysis**: Processing time and token usage
5. **ğŸ“‹ Interactive Dashboards**: Switch between models dynamically

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenRouter**: API access to cutting-edge language models
- **EXPW Dataset**: High-quality emotion recognition data
- **GLM-4.5V**: Advanced reasoning capabilities from Zhipu AI
- **Gemini 2.5 Flash**: Google's fast vision-language model

## ğŸ“ Support

- ğŸ› **Issues**: [GitHub Issues](https://github.com/donghaozhang/emotion-recognition-analysis/issues)
- ğŸ“§ **Contact**: Create an issue for questions or suggestions
- ğŸ“– **Documentation**: Check `CLAUDE.md` for development guidelines

---

**ğŸ¤– Generated with Claude Code** | **â­ Star this repo if you find it useful!**